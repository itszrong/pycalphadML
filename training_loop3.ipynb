{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras import Model, optimizers\n",
    "\n",
    "X = tf.random.shuffle(np.random.normal(size=500).reshape(-1,10,1))\n",
    "# Train_X = X[:30]\n",
    "# Val_X = X[30:40]\n",
    "# Test_X = X[40:50]\n",
    "# Y = 2*X + 2\n",
    "# Train_Y = Y[:30]\n",
    "# Val_Y = Y[30:40]\n",
    "# Test_Y = Y[40:50]\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "Train_X = x_train\n",
    "Train_Y = y_train\n",
    "Val_x = x_test\n",
    "Val_Y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tf.keras.Input(shape=(1,))\n",
    "# x = Dense(2, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "# x = Dense(2, activation=\"relu\", name=\"dense_2\")(x)\n",
    "# outputs = Dense(1, name=\"predictions\")(x)\n",
    "# model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn =mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from getvalues import get_values\n",
    "from neural import CalphadPhaseModel\n",
    "from turtle import shape\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.layers import PolynomialCrossing\n",
    "from keras.layers import Conv1D\n",
    "from keras.optimizers import Optimizer\n",
    "import pycalphad\n",
    "from pycalphad import Database, calculate, variables as v\n",
    "phase_name = 'liquid'\n",
    "dbf = Database('Mg_Si_Zn.tdb')\n",
    "comps = ['MG', 'SI', 'ZN', 'VA']\n",
    "epochs_per_run = 1\n",
    "mod = pycalphad.Model(dbf, comps, phase_name)\n",
    "sublattice_dof = [len(t) for t in mod.constituents]\n",
    "temp_scale = x_train[:, 0].max()\n",
    "energy_scale = y_train.std()\n",
    "model = CalphadPhaseModel(sublattice_dof, mod.site_ratios, name=phase_name,\n",
    "                                 temp_scale=temp_scale, energy_scale=energy_scale)\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "batch_size_train = 100\n",
    "batch_size_val = 20\n",
    "\n",
    "def fetch_batch(X, y, batch_size, batch):\n",
    "    start = batch*batch_size\n",
    "    \n",
    "    X_batch = X[start:start+batch_size, :]\n",
    "    y_batch = y[start:start+batch_size]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "batch = 1\n",
    "\n",
    "X, y = shuffle(x_train, y_train, random_state = 42)\n",
    "X_batch, y_batch = fetch_batch(X, y, batch_size_train, batch)\n",
    "\n",
    "Val_X_batch, Val_y_batch = shuffle(x_test, y_test, random_state = 42)\n",
    "Val_X_batch, Val_y_batch = fetch_batch(Val_X_batch, Val_y_batch, batch_size_val, batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (100,)\n",
      "(20, 4) (20,)\n"
     ]
    }
   ],
   "source": [
    "print(X_batch.shape, y_batch.shape)\n",
    "print(Val_X_batch.shape, Val_y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(x, y, model, get_values, dbf, comps):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # print(step)\n",
    "        x = x[np.newaxis].T\n",
    "        entropy = get_values('SM', 'LIQUID', x.T, dbf, comps)\n",
    "        entropy = tf.convert_to_tensor(entropy, dtype=tf.float32)\n",
    "\n",
    "        x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        tape.watch(x_tensor)\n",
    "        predictions = model(x_tensor, training=True)\n",
    "        # print(predictions)\n",
    "        dy_dx = tape.gradient(predictions, x_tensor)\n",
    "        # print(dy_dx)\n",
    "\n",
    "        RMSE_loss = tf.sqrt(tf.divide(tf.reduce_sum(tf.pow(tf.subtract(predictions, y_tensor),2.0)),tf.cast(tf.size(y_tensor), tf.float32))) \n",
    "        derivative_loss = tf.add(entropy, sum(dy_dx))\n",
    "        loss_value = tf.add(RMSE_loss, derivative_loss)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Epoch:  1  Average Train loss: [22253.86] Average Val loss: [20319.834]\n",
      "Epoch:  2  Average Train loss: [22253.598] Average Val loss: [20317.074]\n",
      "Epoch:  3  Average Train loss: [22253.26] Average Val loss: [20312.73]\n",
      "Epoch:  4  Average Train loss: [22252.97] Average Val loss: [20306.41]\n",
      "Epoch:  5  Average Train loss: [22253.098] Average Val loss: [20299.871]\n",
      "Epoch:  6  Average Train loss: [22253.688] Average Val loss: [20296.348]\n",
      "Epoch:  7  Average Train loss: [22254.074] Average Val loss: [20295.322]\n",
      "Epoch:  8  Average Train loss: [22254.19] Average Val loss: [20295.092]\n",
      "Epoch:  9  Average Train loss: [22254.215] Average Val loss: [20295.037]\n",
      "Epoch:  10  Average Train loss: [22254.217] Average Val loss: [20295.02]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss_array = []\n",
    "    val_loss_array = []\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(zip(X_batch, y_batch)):\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # print(step)\n",
    "            x_batch_train = x_batch_train[np.newaxis].T\n",
    "            entropy = get_values('SM', 'LIQUID', x_batch_train.T, dbf, comps)\n",
    "            entropy = tf.convert_to_tensor(entropy, dtype=tf.float32)\n",
    "\n",
    "            x_tensor = tf.convert_to_tensor(x_batch_train, dtype=tf.float32)\n",
    "            y_tensor = tf.convert_to_tensor(y_batch_train, dtype=tf.float32)\n",
    "            tape.watch(x_tensor)\n",
    "            # print(x_tensor)\n",
    "            predictions = model(x_tensor, training=True)\n",
    "            # print(predictions)\n",
    "\n",
    "            dy_dx = tape.gradient(predictions, x_tensor)\n",
    "\n",
    "            RMSE_loss = tf.sqrt(tf.divide(tf.reduce_sum(tf.pow(tf.subtract(predictions, y_tensor),2.0)),tf.cast(tf.size(y_tensor), tf.float32))) \n",
    "            derivative_loss = tf.add(entropy, sum(dy_dx))\n",
    "            loss_value = tf.add(RMSE_loss, derivative_loss)\n",
    "            # print(loss_value)\n",
    "            loss_array.append(loss_value)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    #print(grads)\n",
    "    # print('Epoch: ', epoch+1,' Average Train loss:', tf.get_static_value(sum(loss_array)/len(loss_array)))\n",
    "\n",
    "    for step, (x_batch_val, y_batch_val) in enumerate(zip(Val_X_batch, Val_y_batch)):\n",
    "        # val_predictions = model(x_batch_val, training=False)\n",
    "        # print('Valuation predictions for the batch :')\n",
    "        # print(val_predictions)\n",
    "        # print('Actual Valuation for the batch :')\n",
    "        # print(y_batch_val)\n",
    "        # print('x_batch_val', x_batch_val)\n",
    "        x, y = x_batch_val, y_batch_val\n",
    "        val_loss_value = get_loss(x, y, model, get_values, dbf, comps)\n",
    "        val_loss_array.append(val_loss_value)\n",
    "\n",
    "        # y_batch_val = tf.cast(y_batch_val, tf.float32)\n",
    "        # RMSE_loss = tf.sqrt(tf.divide(tf.reduce_sum(tf.pow(tf.subtract(predictions, y_tensor),2.0)),tf.cast(tf.size(y_tensor), tf.float32))) \n",
    "        # derivative_loss = tf.add(entropy, sum(dy_dx))\n",
    "        # loss_value = tf.add(RMSE_loss, derivative_loss)\n",
    "        # print(loss_value)\n",
    "    print('Epoch: ', epoch+1, ' Average Train loss:', tf.get_static_value(sum(loss_array)/len(loss_array)), 'Average Val loss:', tf.get_static_value(sum(val_loss_array)/len(val_loss_array)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3.3296045e-02]\n",
      " [3.6291695e+01]\n",
      " [3.5599747e+01]\n",
      " [3.6237251e+01]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zhenyap\\pycalphad\\training_loop.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m shuffle\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000003?line=3'>4</a>\u001b[0m start1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "start1 = time.time()\n",
    "batch_size=100\n",
    "n_epochs = 10\n",
    "model1 = model\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "for epoch in range(n_epochs):\n",
    "    X, y = shuffle(x_train, y_train, random_state = epoch**2)\n",
    "    for batch in tqdm(range(len(x_train) //batch_size)):\n",
    "    \n",
    "        X_batch, y_batch = fetch_batch(X, y, batch_size, batch)\n",
    "        # X_batch, y_batch = fetch_random_batch(X, y, batch_size)\n",
    "        loss, acc = model1.train_on_batch(X_batch, y_batch)\n",
    "    \n",
    "    loss_history.append(loss)\n",
    "    acc_history.append(acc)\n",
    "    \n",
    "    # Run validtion at the end of each epoch.\n",
    "    y_pred = model1.predict(x_test)\n",
    "    val_loss, val_acc = model1.evaluate(x_test, y_test)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "        \n",
    "        \n",
    "    print('Epoch: %d, Train Loss %.3f, Train Acc. %.3f, Val Loss %.3f, Val Acc. %.3f' %\n",
    "\t\t\t(epoch+1, loss, acc, val_loss, val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('calphad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "818be52fc3e67187423b156bcc805b3603ed8f46451ea39369ad69a186d5bd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
