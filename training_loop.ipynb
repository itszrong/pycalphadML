{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from tensorflow.keras import Model, optimizers\n",
    "\n",
    "X = tf.random.shuffle(np.random.normal(size=500).reshape(-1,10,1))\n",
    "# Train_X = X[:30]\n",
    "# Val_X = X[30:40]\n",
    "# Test_X = X[40:50]\n",
    "# Y = 2*X + 2\n",
    "# Train_Y = Y[:30]\n",
    "# Val_Y = Y[30:40]\n",
    "# Test_Y = Y[40:50]\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "Train_X = x_train\n",
    "Train_Y = y_train\n",
    "Val_x = x_test\n",
    "Val_Y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tf.keras.Input(shape=(1,))\n",
    "# x = Dense(2, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "# x = Dense(2, activation=\"relu\", name=\"dense_2\")(x)\n",
    "# outputs = Dense(1, name=\"predictions\")(x)\n",
    "# model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn =mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from getvalues import get_values\n",
    "from neural import CalphadPhaseModel\n",
    "from turtle import shape\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.layers import PolynomialCrossing\n",
    "from keras.layers import Conv1D\n",
    "from keras.optimizers import Optimizer\n",
    "import pycalphad\n",
    "from pycalphad import Database, calculate, variables as v\n",
    "phase_name = 'liquid'\n",
    "dbf = Database('Mg_Si_Zn.tdb')\n",
    "comps = ['MG', 'SI', 'ZN', 'VA']\n",
    "epochs_per_run = 1\n",
    "mod = pycalphad.Model(dbf, comps, phase_name)\n",
    "sublattice_dof = [len(t) for t in mod.constituents]\n",
    "temp_scale = x_train[:, 0].max()\n",
    "energy_scale = y_train.std()\n",
    "model = CalphadPhaseModel(sublattice_dof, mod.site_ratios, name=phase_name,\n",
    "                                 temp_scale=temp_scale, energy_scale=energy_scale)\n",
    "model.compile(optimizer=optimizer, loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "batch_size_train = 100\n",
    "batch_size_val = 20\n",
    "\n",
    "def fetch_batch(X, y, batch_size, batch):\n",
    "    start = batch*batch_size\n",
    "    \n",
    "    X_batch = X[start:start+batch_size, :]\n",
    "    y_batch = y[start:start+batch_size]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "batch = 1\n",
    "\n",
    "X, y = shuffle(x_train, y_train, random_state = 42)\n",
    "X_batch, y_batch = fetch_batch(X, y, batch_size_train, batch)\n",
    "\n",
    "Val_X_batch, Val_y_batch = shuffle(x_test, y_test, random_state = 42)\n",
    "Val_X_batch, Val_y_batch = fetch_batch(Val_X_batch, Val_y_batch, batch_size_val, batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (100,)\n",
      "(20, 4) (20,)\n"
     ]
    }
   ],
   "source": [
    "print(X_batch.shape, y_batch.shape)\n",
    "print(Val_X_batch.shape, Val_y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(x, y, model, get_values, dbf, comps):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # print(step)\n",
    "        x = x[np.newaxis].T\n",
    "        entropy = get_values('SM', 'LIQUID', x.T, dbf, comps)\n",
    "        entropy = tf.convert_to_tensor(entropy, dtype=tf.float32)\n",
    "\n",
    "        x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        tape.watch(x_tensor)\n",
    "        predictions = model(x_tensor, training=True)\n",
    "        # print(predictions)\n",
    "        dy_dx = tape.gradient(predictions, x_tensor)\n",
    "        # print(dy_dx)\n",
    "\n",
    "        RMSE_loss = tf.sqrt(tf.divide(tf.reduce_sum(tf.pow(tf.subtract(predictions, y_tensor),2.0)),tf.cast(tf.size(y_tensor), tf.float32))) \n",
    "        derivative_loss = tf.add(entropy, sum(dy_dx))\n",
    "        loss_value = tf.add(RMSE_loss, derivative_loss)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhenyap\\pycalphad\\pycalphad\\codegen\\callables.py:94: UserWarning: State variables in `build_callables` are not {N, P, T}, but {T}. This can lead to incorrectly calculated values if the state variables used to call the generated functions do not match the state variables used to create them. State variables can be added with the `additional_statevars` argument.\n",
      "  warnings.warn(\"State variables in `build_callables` are not {{N, P, T}}, but {}. This can lead to incorrectly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Epoch:  1  Average Train loss: [nan] Average Val loss: [nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zhenyap\\pycalphad\\training_loop.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000007?line=11'>12</a>\u001b[0m x_batch_train \u001b[39m=\u001b[39m x_batch_train[np\u001b[39m.\u001b[39mnewaxis]\u001b[39m.\u001b[39mT\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000007?line=12'>13</a>\u001b[0m entropy \u001b[39m=\u001b[39m get_values(\u001b[39m'\u001b[39m\u001b[39mSM\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLIQUID\u001b[39m\u001b[39m'\u001b[39m, x_batch_train\u001b[39m.\u001b[39mT, dbf, comps)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000007?line=13'>14</a>\u001b[0m entropy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(entropy, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000007?line=15'>16</a>\u001b[0m x_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(x_batch_train, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000007?line=16'>17</a>\u001b[0m y_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(y_batch_train, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\zhenyap\\Miniconda3\\envs\\calphad\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:140\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_handler\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_traceback_filtering_enabled():\n\u001b[0;32m    141\u001b[0m       \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    142\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39m# In some very rare cases,\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[39m# `is_traceback_filtering_enabled` (from the outer scope) may not be\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[39m# accessible from inside this function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zhenyap\\Miniconda3\\envs\\calphad\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:32\u001b[0m, in \u001b[0;36mis_traceback_filtering_enabled\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m _ENABLE_TRACEBACK_FILTERING \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mlocal()\n\u001b[0;32m     27\u001b[0m _EXCLUDED_PATHS \u001b[39m=\u001b[39m (\n\u001b[0;32m     28\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m__file__\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m)),\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdebugging.is_traceback_filtering_enabled\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_traceback_filtering_enabled\u001b[39m():\n\u001b[0;32m     34\u001b[0m   \u001b[39m\"\"\"Check whether traceback filtering is currently enabled.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[39m  See also `tf.debugging.enable_traceback_filtering()` and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m    was called).\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_ENABLE_TRACEBACK_FILTERING, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss_array = []\n",
    "    val_loss_array = []\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(zip(X_batch, y_batch)):\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # print(step)\n",
    "            x_batch_train = x_batch_train[np.newaxis].T\n",
    "            entropy = get_values('SM', 'LIQUID', x_batch_train.T, dbf, comps)\n",
    "            entropy = tf.convert_to_tensor(entropy, dtype=tf.float32)\n",
    "\n",
    "            x_tensor = tf.convert_to_tensor(x_batch_train, dtype=tf.float32)\n",
    "            y_tensor = tf.convert_to_tensor(y_batch_train, dtype=tf.float32)\n",
    "            tape.watch(x_tensor)\n",
    "            # print(x_tensor)\n",
    "            predictions = model(x_tensor, training=True)\n",
    "            # print(predictions)\n",
    "\n",
    "            dy_dx = tape.gradient(predictions, x_tensor)\n",
    "\n",
    "            RMSE_loss = tf.sqrt(tf.divide(tf.reduce_sum(tf.pow(tf.subtract(predictions, y_tensor),2.0)),tf.cast(tf.size(y_tensor), tf.float32))) \n",
    "            derivative_loss = tf.add(entropy, sum(dy_dx))\n",
    "            loss_value = tf.add(RMSE_loss, derivative_loss)\n",
    "            # print(loss_value)\n",
    "            loss_array.append(loss_value)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    #print(grads)\n",
    "    # print('Epoch: ', epoch+1,' Average Train loss:', tf.get_static_value(sum(loss_array)/len(loss_array)))\n",
    "\n",
    "    for step, (x_batch_val, y_batch_val) in enumerate(zip(Val_X_batch, Val_y_batch)):\n",
    "        # val_predictions = model(x_batch_val, training=False)\n",
    "        # print('Valuation predictions for the batch :')\n",
    "        # print(val_predictions)\n",
    "        # print('Actual Valuation for the batch :')\n",
    "        # print(y_batch_val)\n",
    "        # print('x_batch_val', x_batch_val)\n",
    "        x, y = x_batch_val, y_batch_val\n",
    "        val_loss_value = get_loss(x, y, model, get_values, dbf, comps)\n",
    "        val_loss_array.append(val_loss_value)\n",
    "\n",
    "        # y_batch_val = tf.cast(y_batch_val, tf.float32)\n",
    "        # RMSE_loss = tf.sqrt(tf.divide(tf.reduce_sum(tf.pow(tf.subtract(predictions, y_tensor),2.0)),tf.cast(tf.size(y_tensor), tf.float32))) \n",
    "        # derivative_loss = tf.add(entropy, sum(dy_dx))\n",
    "        # loss_value = tf.add(RMSE_loss, derivative_loss)\n",
    "        # print(loss_value)\n",
    "    print('Epoch: ', epoch+1, ' Average Train loss:', tf.get_static_value(sum(loss_array)/len(loss_array)), 'Average Val loss:', tf.get_static_value(sum(val_loss_array)/len(val_loss_array)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3.3296045e-02]\n",
      " [3.6291695e+01]\n",
      " [3.5599747e+01]\n",
      " [3.6237251e+01]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zhenyap\\pycalphad\\training_loop.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m shuffle\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhenyap/pycalphad/training_loop.ipynb#ch0000003?line=3'>4</a>\u001b[0m start1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "start1 = time.time()\n",
    "batch_size=100\n",
    "n_epochs = 10\n",
    "model1 = model\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "for epoch in range(n_epochs):\n",
    "    X, y = shuffle(x_train, y_train, random_state = epoch**2)\n",
    "    for batch in tqdm(range(len(x_train) //batch_size)):\n",
    "    \n",
    "        X_batch, y_batch = fetch_batch(X, y, batch_size, batch)\n",
    "        # X_batch, y_batch = fetch_random_batch(X, y, batch_size)\n",
    "        loss, acc = model1.train_on_batch(X_batch, y_batch)\n",
    "    \n",
    "    loss_history.append(loss)\n",
    "    acc_history.append(acc)\n",
    "    \n",
    "    # Run validtion at the end of each epoch.\n",
    "    y_pred = model1.predict(x_test)\n",
    "    val_loss, val_acc = model1.evaluate(x_test, y_test)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "        \n",
    "        \n",
    "    print('Epoch: %d, Train Loss %.3f, Train Acc. %.3f, Val Loss %.3f, Val Acc. %.3f' %\n",
    "\t\t\t(epoch+1, loss, acc, val_loss, val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('calphad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "818be52fc3e67187423b156bcc805b3603ed8f46451ea39369ad69a186d5bd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
